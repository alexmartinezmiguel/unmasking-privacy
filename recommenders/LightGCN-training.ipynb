{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef212f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from fastparquet import ParquetFile\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n",
    "from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_stratified_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(model,predicted_ranking, targets):\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    for user_id in targets.user_id.unique():\n",
    "        # we compute recall and ndcg for each user and then aggregate\n",
    "        user_predicted_ranking = predicted_ranking[predicted_ranking.user_id==user_id].item_id.values\n",
    "        user_targets = targets[targets.user_id==user_id].item_id.values\n",
    "        \n",
    "        # compute recall\n",
    "        num_hit = len(set(user_predicted_ranking).intersection(set(user_targets)))\n",
    "        user_recall = float(num_hit) / len(user_targets)\n",
    "        recall.append(user_recall)\n",
    "        \n",
    "        # relevance to compute ndcg (recommendations and ideal)\n",
    "        recom_relevance = list()\n",
    "        for item_id in user_predicted_ranking:\n",
    "            if item_id in user_targets:\n",
    "                recom_relevance.append(1.)\n",
    "            else:\n",
    "                recom_relevance.append(0.)\n",
    "        ideal_relevance = -np.sort(-np.array(recom_relevance))\n",
    "        \n",
    "        # compute ndcg\n",
    "        if np.sum(recom_relevance)==0.0:\n",
    "            ndcg.append(0.0)\n",
    "        else:\n",
    "            recom_dcg = np.sum(recom_relevance/np.log2(1+np.arange(1,len(recom_relevance)+1)))\n",
    "            ideal_dcg = np.sum(ideal_relevance/np.log2(1+np.arange(1,len(ideal_relevance)+1)))\n",
    "            ndcg.append(recom_dcg/ideal_dcg)\n",
    "\n",
    "    recall = np.array(recall).squeeze()\n",
    "    ndcg = np.array(ndcg).squeeze()\n",
    "    \n",
    "    # compute loss function\n",
    "    loss = compute_loss_function(model)\n",
    "\n",
    "    return {'recall':np.mean(recall), 'ndcg':np.mean(ndcg), 'loss':loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_function(model):\n",
    "    # initialize\n",
    "    loss = 0.0\n",
    "\n",
    "    # calculate loss and update NCF parameters\n",
    "    n_batch = model.data.train.shape[0] // model.batch_size + 1\n",
    "    for idx in range(n_batch):\n",
    "        users, pos_items, neg_items = model.data.train_loader(model.batch_size)\n",
    "        \n",
    "        batch_loss = model.sess.run(model.loss, feed_dict={\n",
    "            model.users: users,\n",
    "            model.pos_items: pos_items,\n",
    "            model.neg_items: neg_items\n",
    "        })\n",
    "\n",
    "\n",
    "        # get loss and execute optimization\n",
    "        loss += batch_loss / n_batch\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89712af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(pred_df, k, column):\n",
    "    \"\"\"\n",
    "    Get the top-k items for each user according to the prediction score\n",
    "    \"\"\"\n",
    "    top_k_df = pred_df.groupby('userID').apply(lambda x: x.nlargest(k, column)).reset_index(drop=True)\n",
    "    return top_k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903da010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_recommend(users,training_df,k):\n",
    "    predictions_df = pd.DataFrame(columns=['userID','itemID','prediction'])\n",
    "    for user_id in users:\n",
    "        items_to_sample = set(all_items).difference(training_df[training_df['userID']==user_id].itemID.unique())\n",
    "        items = random.sample(list(items_to_sample), k)\n",
    "        tmp_df = pd.DataFrame(columns=['userID','itemID','prediction'])\n",
    "        tmp_df['itemID'] = list(items)\n",
    "        tmp_df['userID'] = user_id\n",
    "        tmp_df['prediction'] = 1.0\n",
    "        predictions_df = pd.concat([predictions_df, tmp_df.sort_values('prediction', ascending=False)[:k]])\n",
    "    return predictions_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f11d8",
   "metadata": {},
   "source": [
    "### MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915069af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '1m'\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results\n",
    "\n",
    "yaml_file = \"lightgcn.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8808a",
   "metadata": {},
   "source": [
    "### TRAIN ORIGINAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755946b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/original_dataset/train_df.csv')\n",
    "all_items = list(train.item_id.unique())\n",
    "test = pd.read_csv('data/original_dataset/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rating'] = 1\n",
    "test['rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56550acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.item_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'user_id':'userID','item_id':'itemID'})\n",
    "test = test.rename(columns={'user_id':'userID','item_id':'itemID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00dd99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "privacy_type = 'original_dataset' \n",
    "# directory = f'data/privacy/{privacy_type}'\n",
    "output_directory = f'output/{privacy_type}'\n",
    "rec_filename = f'LightGCN-{privacy_type}'\n",
    "data = ImplicitCF(train=train, test=test, seed=SEED)\n",
    "\n",
    "hparams = prepare_hparams(yaml_file,\n",
    "                          n_layers=3,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=EPOCHS,\n",
    "                          learning_rate=0.005,\n",
    "                          eval_epoch=5,\n",
    "                          top_k=TOP_K,\n",
    "                         )\n",
    "\n",
    "model = LightGCN(hparams, data, seed=SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model.fit()\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "\n",
    "# predict\n",
    "topk_scores = model.recommend_k_items(test, top_k=TOP_K, remove_seen=True)\n",
    "\n",
    "#output_filename = f'model-{rec_filename}'\n",
    "#model.save(dir_name=os.path.join(output_directory, 'models', output_filename))\n",
    "\n",
    "# save recommendations\n",
    "output_filename = f'recommendations-{rec_filename}.parq'\n",
    "topk_scores.to_parquet(os.path.join(output_directory, 'recommendations', output_filename), \n",
    "                               engine='fastparquet')\n",
    "\n",
    "del model\n",
    "topk_scores = topk_scores.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "test_to_eval = test.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "\n",
    "# evaluation metrics\n",
    "metrics = evaluation_metrics(topk_scores, test_to_eval)\n",
    "output_filename = f'evaluation-metrics-{rec_filename}.json'\n",
    "with open(os.path.join(output_directory, 'evaluation-metrics' ,output_filename), \"w\") as fp:\n",
    "    json.dump(metrics, fp)\n",
    "\n",
    "del topk_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbbcfb",
   "metadata": {},
   "source": [
    "### TRAIN PRIVACY ENHANCED DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45967789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data privacy\n",
    "privacy_type = 'with_mask/fixed-response' \n",
    "directory = f'data/{privacy_type}'\n",
    "output_directory = f'output/privacy_data/{privacy_type}'\n",
    "rec_filename = f'LightGCN-{privacy_type}'\n",
    "\n",
    "groups_to_train = ['1','2','3','5','7','10','11','13','17','19']\n",
    "thetas_to_train = ['0.2','0.4','0.6','0.8']\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_name = os.path.join(directory, filename)\n",
    "    # get the type of data from filename\n",
    "    data_type = filename.split('.parq')[0]\n",
    "    group = data_type.split('groups')[1].split('_')[0]\n",
    "    theta = data_type.split('groups')[1].split('theta')[1].split('_')[0]\n",
    "    print(group, theta)\n",
    "    if group in groups_to_train and theta in thetas_to_train:\n",
    "        # load test every time\n",
    "        test = pd.read_csv('data/original_dataset/test_df.csv')\n",
    "        test = test.rename(columns={'user_id':'userID','item_id':'itemID'})\n",
    "        test['rating'] = 1\n",
    "        # read parquet, convert to pandas and filter positive interactions\n",
    "        parq_df = ParquetFile(file_name)\n",
    "        pd_df = parq_df.to_pandas()\n",
    "        train_df = pd_df[pd_df['interaction_r']==1][['user_id','item_id','interaction_r']].rename(columns={'user_id':'userID','item_id':'itemID','interaction_r':'rating'})\n",
    "        if train_df.isna().sum().sum()!=0:\n",
    "            #nan_userid.setdefault(data_type, train_df[train_df.isnull().any(axis=1)].userID.unique())\n",
    "            train_df.dropna(inplace=True)\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        # we need to make sure that we have not added interactions that are part of the test set\n",
    "        tmp = train_df.merge(test[['userID','itemID','rating']], on=['userID','itemID','rating'])\n",
    "        train_df = pd.merge(train_df, tmp, on=['userID','itemID','rating'], how='outer', indicator=True).query(\"_merge != 'both'\").drop('_merge', axis=1).reset_index(drop=True)\n",
    "        data = ImplicitCF(train=train_df, seed=SEED)\n",
    "        hparams = prepare_hparams(yaml_file,\n",
    "                                  n_layers=3,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  learning_rate=0.005,\n",
    "                                  eval_epoch=-1,\n",
    "                                  top_k=TOP_K,\n",
    "                                 )\n",
    "        # create model\n",
    "        model = LightGCN(hparams, data, seed=SEED)\n",
    "        \n",
    "        # train\n",
    "        with Timer() as train_time:\n",
    "            model.fit()\n",
    "        print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "        \n",
    "        # predict\n",
    "        test_specific_users = test[test.userID.isin(train_df.userID.unique())]\n",
    "        topk_scores = model.recommend_k_items(test_specific_users, top_k=TOP_K, remove_seen=True)\n",
    "        # for other users not present in train_df, give random recommendations\n",
    "        users_random = list(set(test.userID.unique()).difference(train_df.userID.unique()))\n",
    "        topk_scores_random = random_recommend(users_random, train_df,TOP_K)\n",
    "        topk_scores = pd.concat([topk_scores, topk_scores_random], ignore_index=True, sort=False)\n",
    "        #topk_scores.replace({'userID':reverse_map_dict_users}, inplace=True)\n",
    "        #test.replace({'userID':reverse_map_dict_users}, inplace=True)\n",
    "\n",
    "        # save recommendations\n",
    "        output_filename = f'recommendations-LightGCN-{data_type}.parq'\n",
    "        topk_scores.to_parquet(os.path.join(output_directory, 'recommendations', output_filename), \n",
    "                               engine='fastparquet')\n",
    "\n",
    "        topk_scores = topk_scores.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "        test_to_eval = test.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "\n",
    "        # evaluation metrics\n",
    "        metrics = evaluation_metrics(model,topk_scores, test_to_eval)\n",
    "        output_filename = f'evaluation-metrics-LightGCN-{data_type}.json'\n",
    "        with open(os.path.join(output_directory, 'evaluation-metrics' ,output_filename), \"w\") as fp:\n",
    "            json.dump(metrics, fp)\n",
    "\n",
    "        del topk_scores\n",
    "        del model\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65065e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lime-rs",
   "language": "python",
   "name": "lime-rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
