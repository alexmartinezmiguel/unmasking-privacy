{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef212f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.ncf.ncf_singlenode import NCF\n",
    "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.utils.notebook_utils import is_jupyter\n",
    "from recommenders.datasets.python_splitters import python_chrono_split\n",
    "from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
    "                                                     recall_at_k, get_top_k_items)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(model,predicted_ranking, targets):\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    for user_id in targets.user_id.unique():\n",
    "        # we compute recall and ndcg for each user and then aggregate\n",
    "        user_predicted_ranking = predicted_ranking[predicted_ranking.user_id==user_id].item_id.values\n",
    "        user_targets = targets[targets.user_id==user_id].item_id.values\n",
    "        \n",
    "        # compute recall\n",
    "        num_hit = len(set(user_predicted_ranking).intersection(set(user_targets)))\n",
    "        user_recall = float(num_hit) / len(user_targets)\n",
    "        recall.append(user_recall)\n",
    "        \n",
    "        # relevance to compute ndcg (recommendations and ideal)\n",
    "        recom_relevance = list()\n",
    "        for item_id in user_predicted_ranking:\n",
    "            if item_id in user_targets:\n",
    "                recom_relevance.append(1.)\n",
    "            else:\n",
    "                recom_relevance.append(0.)\n",
    "        ideal_relevance = -np.sort(-np.array(recom_relevance))\n",
    "        \n",
    "        # compute ndcg\n",
    "        if np.sum(recom_relevance)==0.0:\n",
    "            ndcg.append(0.0)\n",
    "        else:\n",
    "            recom_dcg = np.sum(recom_relevance/np.log2(1+np.arange(1,len(recom_relevance)+1)))\n",
    "            ideal_dcg = np.sum(ideal_relevance/np.log2(1+np.arange(1,len(ideal_relevance)+1)))\n",
    "            ndcg.append(recom_dcg/ideal_dcg)\n",
    "\n",
    "    recall = np.array(recall).squeeze()\n",
    "    ndcg = np.array(ndcg).squeeze()\n",
    "    \n",
    "    # compute loss function\n",
    "    loss = compute_loss_function(model)\n",
    "\n",
    "    return {'recall':np.mean(recall), 'ndcg':np.mean(ndcg), 'loss':loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89712af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(pred_df, k, column):\n",
    "    \"\"\"\n",
    "    Get the top-k items for each user according to the prediction score\n",
    "    \"\"\"\n",
    "    top_k_df = pred_df.groupby('userID').apply(lambda x: x.nlargest(k, column)).reset_index(drop=True)\n",
    "    return top_k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903da010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_recommend(users,training_df,k):\n",
    "    predictions_df = pd.DataFrame(columns=['userID','itemID','prediction'])\n",
    "    for user_id in users:\n",
    "        items_to_sample = set(all_items).difference(training_df[training_df['userID']==user_id].itemID.unique())\n",
    "        items = random.sample(list(items_to_sample), k)\n",
    "        tmp_df = pd.DataFrame(columns=['userID','itemID','prediction'])\n",
    "        tmp_df['itemID'] = list(items)\n",
    "        tmp_df['userID'] = user_id\n",
    "        tmp_df['prediction'] = 1.0\n",
    "        predictions_df = pd.concat([predictions_df, tmp_df.sort_values('prediction', ascending=False)[:k]])\n",
    "    return predictions_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_function(model):\n",
    "    # initialize\n",
    "    train_loss = []\n",
    "\n",
    "    # calculate loss and update NCF parameters\n",
    "    for user_input, item_input, labels in data.train_loader(model.batch_size):\n",
    "\n",
    "        user_input = np.array([model.user2id[x] for x in user_input])\n",
    "        item_input = np.array([model.item2id[x] for x in item_input])\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        feed_dict = {\n",
    "            model.user_input: user_input[..., None],\n",
    "            model.item_input: item_input[..., None],\n",
    "            model.labels: labels[..., None],\n",
    "        }\n",
    "\n",
    "        # get loss and execute optimization\n",
    "        loss = model.sess.run(model.loss, feed_dict)\n",
    "        train_loss.append(loss)\n",
    "        \n",
    "    return sum(train_loss) / len(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f11d8",
   "metadata": {},
   "source": [
    "### MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915069af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588620b",
   "metadata": {},
   "source": [
    "### TRAIN ORIGINAL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755946b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/original_dataset/train_df.csv')\n",
    "all_items = list(train.item_id.unique())\n",
    "test = pd.read_csv('data/original_dataset/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rating'] = 1\n",
    "test['rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'user_id':'userID','item_id':'itemID'})\n",
    "test = test.rename(columns={'user_id':'userID','item_id':'itemID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49001af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values('userID')\n",
    "test = test.sort_values('userID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba713aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"MS_data/train.csv\"\n",
    "test_file = \"MS_data/test.csv\"\n",
    "#train.to_csv(train_file, index=False)\n",
    "#test.to_csv(test_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2130c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_type = 'original_dataset' \n",
    "output_directory = f'output/{privacy_type}'\n",
    "rec_filename = f'NCF-{privacy_type}'\n",
    "\n",
    "data = NCFDataset(train_file=train_file, seed=SEED)\n",
    "\n",
    "model = NCF(\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model.fit(data)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time))\n",
    "\n",
    "with Timer() as test_time:\n",
    "    users, items, preds = [], [], []\n",
    "    item = list(train.itemID.unique())\n",
    "    for user in train.userID.unique():\n",
    "        user = [user] * len(item) \n",
    "        users.extend(user)\n",
    "        items.extend(item)\n",
    "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
    "\n",
    "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time))\n",
    "\n",
    "reduced_predictions = get_top_k(all_predictions,10,'prediction')\n",
    "\n",
    "output_filename = f'model-{rec_filename}'\n",
    "model.save(dir_name=output_filename)\n",
    "    \n",
    "# save recommendations\n",
    "output_filename = f'recommendations-{rec_filename}.parq'\n",
    "reduced_predictions.to_parquet(os.path.join(output_directory, 'recommendations', output_filename), \n",
    "                               engine='fastparquet')\n",
    "\n",
    "\n",
    "reduced_predictions = reduced_predictions.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "test_to_eval = test.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "\n",
    "\n",
    "# evaluation metrics\n",
    "metrics = evaluation_metrics(model,reduced_predictions, test_to_eval)\n",
    "output_filename = f'evaluation-metrics-{privacy_type}.json'\n",
    "with open(os.path.join(output_directory, 'evaluation-metrics' ,output_filename), \"w\") as fp:\n",
    "    json.dump(metrics, fp)\n",
    "del model\n",
    "del reduced_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15d190",
   "metadata": {},
   "source": [
    "### TRAIN PRIVACY ENHANCED DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45967789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data privacy\n",
    "for response in ['fixed']:\n",
    "    privacy_type = f'with_mask/{response}-response' \n",
    "    print(privacy_type)\n",
    "    directory = f'data/{privacy_type}'\n",
    "    output_directory = f'output/privacy_data/{privacy_type}'\n",
    "    rec_filename = f'NCF-{privacy_type}'\n",
    "\n",
    "    groups_to_train = ['1','2','3','5','7','10','11','13','17','19']\n",
    "    thetas_to_train = ['0.2','0.4','0.6','0.8']\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        file_name = os.path.join(directory, filename)\n",
    "        # get the type of data from filename\n",
    "        data_type = filename.split('.parq')[0]\n",
    "        group = data_type.split('groups')[1].split('_')[0]\n",
    "        theta = data_type.split('groups')[1].split('theta')[1].split('_')[0]\n",
    "        #print(group, theta)\n",
    "        combination = (group, theta)\n",
    "        if group in groups_to_train and theta in thetas_to_train:\n",
    "            print(group, theta)\n",
    "            # read parquet, convert to pandas and filter positive interactions\n",
    "            parq_df = ParquetFile(file_name)\n",
    "            pd_df = parq_df.to_pandas()\n",
    "            train_df = pd_df[pd_df['interaction_r']==1][['user_id','item_id','interaction_r']].rename(columns={'user_id':'userID','item_id':'itemID','interaction_r':'rating'})\n",
    "            if train_df.isna().sum().sum()!=0:\n",
    "                train_df.dropna(inplace=True)\n",
    "            # make sure to not include test interactions in the training set\n",
    "            tmp = train_df.merge(test[['userID','itemID','rating']], on=['userID','itemID','rating'])\n",
    "            train_df = pd.merge(train_df, tmp, on=['userID','itemID','rating'], how='outer', indicator=True).query(\"_merge != 'both'\").drop('_merge', axis=1).reset_index(drop=True)\n",
    "            train_df = train_df.sort_values('userID')\n",
    "            train_df['userID'] = train_df.userID.astype(int)\n",
    "\n",
    "\n",
    "            # create train/test files\n",
    "            train_file = f\"MS_data/train-{data_type}.csv\"\n",
    "            test_file = f\"MS_data/test-{data_type}.csv\"\n",
    "            train_df.to_csv(train_file, index=False)\n",
    "            test.to_csv(test_file, index=False)\n",
    "\n",
    "            data = NCFDataset(train_file=train_file, seed=SEED)\n",
    "            model = NCF(\n",
    "                n_users=data.n_users, \n",
    "                n_items=data.n_items,\n",
    "                model_type=\"NeuMF\",\n",
    "                n_factors=4,\n",
    "                layer_sizes=[16,8,4],\n",
    "                n_epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                learning_rate=1e-3,\n",
    "                verbose=10,\n",
    "                seed=SEED\n",
    "            )\n",
    "\n",
    "            # train\n",
    "            with Timer() as train_time:\n",
    "                model.fit(data)\n",
    "            print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "\n",
    "            # predict\n",
    "            with Timer() as test_time:\n",
    "                users, items, preds = [], [], []\n",
    "                item = list(train_df.itemID.unique())\n",
    "                for user in train_df.userID.unique():\n",
    "                    user = [user] * len(item) \n",
    "                    users.extend(user)\n",
    "                    items.extend(item)\n",
    "                    preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "                all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
    "\n",
    "                merged = pd.merge(train_df, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "                all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "            print(\"Took {} seconds for prediction.\".format(test_time))\n",
    "\n",
    "            reduced_predictions = get_top_k(all_predictions,10,'prediction')\n",
    "            # for other users not present in train_df, give random recommendations\n",
    "            users_random = list(set(test.userID.unique()).difference(train_df.userID.unique()))\n",
    "            topk_scores_random = random_recommend(users_random, train_df,10)\n",
    "            reduced_predictions = pd.concat([reduced_predictions, topk_scores_random], ignore_index=True, sort=False)\n",
    "            reduced_predictions = reduced_predictions.sort_values('userID')\n",
    "\n",
    "            # save model\n",
    "            output_filename = f'model-NCF-{data_type}'\n",
    "            model.save(dir_name=os.path.join(output_directory, 'models', output_filename))\n",
    "\n",
    "            # save recommendations\n",
    "            output_filename = f'recommendations-NCF-{data_type}.parq'\n",
    "            reduced_predictions.to_parquet(os.path.join(output_directory, 'recommendations', output_filename), \n",
    "                                           engine='fastparquet')\n",
    "\n",
    "            reduced_predictions = reduced_predictions.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "            test_to_eval = test.rename(columns={'userID':'user_id', 'itemID':'item_id'})\n",
    "\n",
    "            # evaluation metrics\n",
    "            metrics = evaluation_metrics(model,reduced_predictions, test_to_eval)\n",
    "            output_filename = f'evaluation-metrics-NCF-{data_type}.json'\n",
    "            with open(os.path.join(output_directory, 'evaluation-metrics' ,output_filename), \"w\") as fp:\n",
    "                json.dump(metrics, fp)\n",
    "\n",
    "            del reduced_predictions\n",
    "            del model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lime-rs",
   "language": "python",
   "name": "lime-rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
